{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrd_WQrYCcPp"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pymorphy2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "alTSVpCj_p3-"
   },
   "source": [
    "В requests.get копируем url требуемого писателя 19-го века. Например Гоголь\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rn4thgAdCErx"
   },
   "outputs": [],
   "source": [
    "tuttoarr2 = []\n",
    "r  = requests.get(\"http://az.lib.ru/p/pushkin_a_s/text_0430.shtml\",   )\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data)\n",
    "inputTag = soup.findAll(\"dl\")\n",
    "\n",
    "for link in inputTag:               \n",
    "                    try:                        \n",
    "                            title = link.findChildren(\"a\",recursive=True)\n",
    "                            for titlea in title: \n",
    "                               if titlea.has_attr('href') and not titlea.has_attr('class'):\n",
    "                                   if 'comment' not in titlea['href']:\n",
    "                        \n",
    "                                        tuttoarr2.append(tuple((titlea['href'],titlea.text)))\n",
    "                    except IndexError:\n",
    "                        print('Rien')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNiuXzvqGbez"
   },
   "outputs": [],
   "source": [
    "newarr=[]\n",
    "for elem in range(1):\n",
    "    r  = requests.get('http://az.lib.ru/p/pushkin_a_s/text_0430.shtml'  )\n",
    "    data = r.text\n",
    "    soup2 = BeautifulSoup(data)\n",
    "    inputTag2 = soup2.findAll(\"div\", {\"align\":\"justify\"})\n",
    "    for link in inputTag2:\n",
    "\n",
    "                        try:\n",
    "\n",
    "                                title = link.findChildren(\"dd\",recursive=True)\n",
    "                                for titlea in title: \n",
    "                                   \n",
    "\n",
    "                                            newarr.append(titlea.text)\n",
    "                                            \n",
    "                        except IndexError:\n",
    "                            print('Rien')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yj4v2sxhNLNa"
   },
   "outputs": [],
   "source": [
    "det = pd.DataFrame(newarr, columns=['text'])\n",
    "det.to_csv('data/teramshort.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "BoHnGCiU1pjW",
    "outputId": "015be091-41e3-4030-ab19-d6ef8c1ae5de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text\n",
       "count     769\n",
       "unique    768\n",
       "top        \\n\n",
       "freq        2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(lines, min_length=1, max_length=3):\n",
    "    lengths = range(min_length, max_length + 1)\n",
    "    ngrams = {length: collections.Counter() for length in lengths}\n",
    "    queue = collections.deque(maxlen=max_length)\n",
    "\n",
    "    def add_queue():\n",
    "        current = tuple(queue)\n",
    "        for length in lengths:\n",
    "            if len(current) >= length:\n",
    "                ngrams[length][current[:length]] += 1\n",
    "\n",
    "    for line in lines:\n",
    "        for word in tokenize(line):\n",
    "            queue.append(word)\n",
    "            if len(queue) >= max_length:\n",
    "                add_queue()\n",
    "\n",
    "    while len(queue) > min_length:\n",
    "        queue.popleft()\n",
    "        add_queue()\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lemmatizer_ru(sent):\n",
    "#     sent = ' '.join(sent)\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "    tokenized_sent = sent.split()\n",
    "    return ' '.join([lemmatizer.parse(word)[0].normal_form\n",
    "                    for word in tokenized_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    #re.findall(r'[А-Яа-я]+', string.lower()))\n",
    "    return string.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_frequent(ngrams, num=10):\n",
    "    for n in sorted(ngrams, reverse=False):\n",
    "        print('----- {} most common {}-grams -----'.format(num, n))\n",
    "        for gram, count in ngrams[n].most_common(num)[:-num:-1]:\n",
    "            print('{0}: {1}'.format(' '.join(gram), count))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenize(det.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 10 most common 1-grams -----\n",
      "Я: 345\n",
      "что: 380\n",
      "я: 430\n",
      "с: 448\n",
      "на: 466\n",
      "--: 497\n",
      "не: 599\n",
      "\n",
      "  : 691\n",
      "в: 824\n",
      "\n",
      "----- 10 most common 2-grams -----\n",
      "не мог: 29\n",
      "я не: 29\n",
      "\n",
      "   Стр.: 36\n",
      "и с: 37\n",
      "-- сказал: 40\n",
      " : 41\n",
      "и не: 43\n",
      "Марья Ивановна: 57\n",
      "\n",
      "   Я: 63\n",
      "\n",
      "----- 10 most common 3-grams -----\n",
      "Петр Андреич! --: 9\n",
      "\n",
      "   На другой: 9\n",
      "\n",
      "   -- Не: 9\n",
      "\n",
      "   -- Да: 9\n",
      "На другой день: 10\n",
      "\n",
      "   -- Что: 11\n",
      ". . .: 11\n",
      "В эту минуту: 14\n",
      "\n",
      "   -- А: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ngrams = count_ngrams(det.text[0].split(' '))\n",
    "ngrams = count_ngrams(a)\n",
    "print_most_frequent(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_corpus_word = []\n",
    "old_corpus_freq = []\n",
    "for i in range(0, 3):\n",
    "    for k, v in ngrams[i+1].items():\n",
    "        old_corpus_word.append(' '.join(k))\n",
    "        old_corpus_freq.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_file(file):\n",
    "    a = []\n",
    "    with open(file, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile, delimiter='\\n')\n",
    "        for row in reader:\n",
    "            a.append(row)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dic = down_file('data/k3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dic = []\n",
    "def_dic = []\n",
    "for i in range(0, len(full_dic)):\n",
    "    word_dic.append(full_dic[i][0].split(' – ')[0].lower())\n",
    "    def_dic.append(full_dic[i][0].split(' – ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "алтын старинная русская монета достоинством в три копейки.\n",
      "армяк крестьянский кафтан; верхняя долгополая одежда в виде халата или кафтана из сукна или грубой шерстяной материи.\n",
      "артикул воинский устав.\n",
      "барин 1. Дворянин, землевладелец, помещик;2. Господин, владелец, хозяин.\n",
      "в случае в милости у влиятельного лица, под покровительством влиятельных лиц. Ср. у И. А. Крылова\n",
      "галоп популярный бальный танец XIX века под музыку размером в две четверти (по счёту раз, два).\n",
      "дабы чтобы; для того, чтобы.\n",
      "камер-лакей придворная должность: старший лакей при царском дворе.\n",
      "капрал лицо, имевшее первый после рядового военный чин; унтер-офицер.\n",
      "кибитка зимняя крытая повозка на полозьях.\n",
      "лучина тонкая длинная щепка, использовавшаяся для освещения помещения.\n",
      "мусье (искаж. франц. monsieur)\n",
      "облучок сидение для кучера в повозке.\n",
      "оный тот; тот самый.\n",
      "оттоле оттуда.\n",
      "подать подушный налог, взимавшийся с крестьян.\n",
      "поручик младший офицерский чин.\n",
      "трактир гостиница с рестораном; ресторан низшего разряда.\n",
      "холоп дворовый, крепостной слуга.\n",
      "час от часу с каждым проходящим часом (для обозначения постепенности усиления).\n",
      "штоф стеклянный четырехугольный сосуд с коротким горлом; мера, содержащая в себе кружку, или восьмую часть ведра.\n",
      "ямщик возница, правящий лошадьми при перевозке на почтовых лошадях пассажиров и грузов.\n"
     ]
    }
   ],
   "source": [
    "# WordDescription.create(id: 1, word: \"текст\", description: \"описание слова текст\")\n",
    "output = []\n",
    "# output_desc = []\n",
    "for i in range(0, len(word_dic)):\n",
    "    for j in range(0, len(old_corpus_word)):\n",
    "        if ''.join(word_dic[i].split(' ')) == ''.join(old_corpus_word[j].split(' ')) :\n",
    "            output.append([' '.join(old_corpus_word[j].split(' ')), \n",
    "                           ' '.join(def_dic[i].split(' '))])\n",
    "            #output_desc.append('WordDescription.create(id: '+str(i)+', word: \\''+\n",
    "            #                  ' '.join(old_corpus_word[j].split(' '))+'\\''+', description: \\''+\n",
    "            #                  ' '.join(def_dic[i].split(' '))+'\\'')\n",
    "            print(old_corpus_word[j], def_dic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(output)):\n",
    "    for j in range(0, len(a)):\n",
    "        if output[i][0] == a[j]:\n",
    "            s = \"<a class='show_description_link' data-remote='true' data-method='get' href='/dhh/word.json?word=\"+str(i)+\"'>\"+a[j]+\"</a>\"\n",
    "            a[j] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[19][0] == ' час от часу '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "b.append(' '.join(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(output)\n",
    "output.to_csv('data/output.txt', index=True, header=False)\n",
    "\n",
    "\n",
    "b = pd.DataFrame(b)\n",
    "b.to_csv('data/text.txt', index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           алтын\n",
       "1           армяк\n",
       "2         артикул\n",
       "3           барин\n",
       "4        в случае\n",
       "5           галоп\n",
       "6            дабы\n",
       "7     камер-лакей\n",
       "8          капрал\n",
       "9         кибитка\n",
       "10         лучина\n",
       "11          мусье\n",
       "12        облучок\n",
       "13           оный\n",
       "14         оттоле\n",
       "15         подать\n",
       "16        поручик\n",
       "17        трактир\n",
       "18          холоп\n",
       "19    час от часу\n",
       "20           штоф\n",
       "21          ямщик\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_desc = []\n",
    "for i in range(0, len(output[0])):\n",
    "    output_desc.append('WordDescription.create(id: '+str(i)+', word: \\''+\n",
    "                              output[0][i]+'\\''+', description: \\''+\n",
    "                              output[1][i]+'\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_desc = pd.DataFrame(output_desc)\n",
    "output_desc.to_csv('data/output_desc.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "baselinetliter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
